{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CwSVBuG-E6X6"
      },
      "outputs": [],
      "source": [
        "# MODULES IMPORT\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pdb\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.stem import SnowballStemmer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GOOGLE COLAB SETUP\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WrOm2qLGhN1m",
        "outputId": "39842aa4-d9ee-4795-9496-c2bc3f1d38be"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing Data\n",
        "\n",
        "train = pd.read_csv('/content/drive/MyDrive/SJ-DECEN_code/data/train.csv')\n",
        "test = pd.read_csv('/content/drive/MyDrive/SJ-DECEN_code/data/test.csv')\n",
        "test_labels = pd.read_csv('/content/drive/MyDrive/SJ-DECEN_code/data/test_labels.csv')"
      ],
      "metadata": {
        "id": "gcbcN83hiwhc"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ilY_NUAI3NV"
      },
      "outputs": [],
      "source": [
        "#Combine labels and test set\n",
        "test                = test.merge(labels, how=\"left\", on=\"id\")\n",
        "test['used_for_sc'] = test[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) >= 0\n",
        "test_labelled       = test[test.used_for_sc==True]\n",
        "test_unlabelled     = test[test.used_for_sc!=True]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VhBEDM5JLots"
      },
      "outputs": [],
      "source": [
        "# Clean data completely\n",
        "def substitute_repeats_fixed_len(text, nchars, ntimes=3):\n",
        "    return re.sub(r\"(\\S{{{}}})(\\1{{{},}})\".format(nchars, ntimes-1), r\"\\1\", text)\n",
        "\n",
        "def substitute_repeats(text, ntimes=3):\n",
        "    for nchars in range(1, 20):\n",
        "        text         = substitute_repeats_fixed_len(text, nchars, ntimes)\n",
        "    return text\n",
        "\n",
        "def text_to_wordlist(text, remove_stop_words=True, stem_words=False, with_punct_sent=False):\n",
        "    stop_words       = set(['a', 'the', \"an\", \"are\", \"as\",  'did',\n",
        "                       \"do\", \"is\", \"has\", \"have\", \"had\", \"was\", \"were\",\n",
        "                       \"will\", \"would\", \"am\", \"it\", \"for\", \"on\", \"it\", \"of\"])\n",
        "    #from string import punctuation\n",
        "    import re\n",
        "    NEG_CONTRACTIONS    = [\n",
        "                          (r'aren\\'t', 'are not'),\n",
        "                          (r'can\\'t', 'can not'),\n",
        "                          (r'couldn\\'t', 'could not'),\n",
        "                          (r'daren\\'t', 'dare not'),\n",
        "                          (r'didn\\'t', 'did not'),\n",
        "                          (r'doesn\\'t', 'does not'),\n",
        "                          (r'don\\'t', 'do not'),\n",
        "                          (r'isn\\'t', 'is not'),\n",
        "                          (r'hasn\\'t', 'has not'),\n",
        "                          (r'haven\\'t', 'have not'),\n",
        "                          (r'hadn\\'t', 'had not'),\n",
        "                          (r'mayn\\'t', 'may not'),\n",
        "                          (r'mightn\\'t', 'might not'),\n",
        "                          (r'mustn\\'t', 'must not'),\n",
        "                          (r'needn\\'t', 'need not'),\n",
        "                          (r'oughtn\\'t', 'ought not'),\n",
        "                          (r'shan\\'t', 'shall not'),\n",
        "                          (r'shouldn\\'t', 'should not'),\n",
        "                          (r'wasn\\'t', 'was not'),\n",
        "                          (r'weren\\'t', 'were not'),\n",
        "                          (r'won\\'t', 'will not'),\n",
        "                          (r'wouldn\\'t', 'would not'),\n",
        "                          (r'ain\\'t', 'am not') # not only but stopword anyway\n",
        "                          ]\n",
        "    OTHER_CONTRACTIONS = [\n",
        "                          #(r\"'m\", 'am'),\n",
        "                          (r\"'ll\", ' will'),\n",
        "                          (r\"'s\", ' has'), # or 'is' but both are stopwords\n",
        "                          (r\"'d\", ' had'), # or 'would' but both are stopwords\n",
        "                          (r\"'ve\", \" have\"),\n",
        "                           (r\"'re\", \" are\")\n",
        "    ]\n",
        "    OTHER_RPS          = [\n",
        "                          (\"&lt;3\", \" good \"),\n",
        "                          (\":d\", \" good \"),\n",
        "                          (\":dd\", \" good \"),\n",
        "                          (\":p\", \" good \"),\n",
        "                          (\"8)\", \" good \"),\n",
        "                          (\":-)\", \" good \"),\n",
        "                          (\":)\", \" good \"),\n",
        "                          (\";)\", \" good \"),\n",
        "                          (\"(-:\", \" good \"),\n",
        "                          (\"(:\", \" good \"),\n",
        "                          (\"yay!\", \" good \"),\n",
        "                          (\"yay\", \" good \"),\n",
        "                          (\"yaay\", \" good \"),\n",
        "                          (\"yaaay\", \" good \"),\n",
        "                          (\"yaaaay\", \" good \"),\n",
        "                          (\"yaaaaay\", \" good \"),\n",
        "                          (\":/\", \" bad \"),\n",
        "                          (\":&gt;\", \" sad \"),\n",
        "                          (\":')\", \" sad \"),\n",
        "                          (\":-(\", \" bad \"),\n",
        "                          (\":(\", \" bad \"),\n",
        "                          (\":s\", \" bad \"),\n",
        "                          (\":-s\", \" bad \"),\n",
        "                          (\"&lt;3\", \" heart \"),\n",
        "                          (\":d\", \" smile \"),\n",
        "                          (\":p\", \" smile \"),\n",
        "                          (\":dd\", \" smile \"),\n",
        "                          (\"8)\", \" smile \"),\n",
        "                          (\":-)\", \" smile \"),\n",
        "                          (\":)\", \" smile \"),\n",
        "                          (\";)\", \" smile \"),\n",
        "                          (\"(-:\", \" smile \"),\n",
        "                          (\"(:\", \" smile \"),\n",
        "                          (\":/\", \" worry \"),\n",
        "                          (\":&gt;\", \" angry \"),\n",
        "                          (\":')\", \" sad \"),\n",
        "                          (\":-(\", \" sad \"),\n",
        "                          (\":(\", \" sad \"),\n",
        "                          (\":s\", \" sad \"),\n",
        "                          (\":-s\", \" sad \"),\n",
        "                          (r\"\\br\\b\", \"are\"),\n",
        "                          (r\"\\bu\\b\", \"you\"),\n",
        "                          (r\"\\bhaha\\b\", \"ha\"),\n",
        "                          (r\"\\bhahaha\\b\", \"ha\"),\n",
        "                          (r\"\\bdon't\\b\", \"do not\"),\n",
        "                          (r\"\\bdoesn't\\b\", \"does not\"),\n",
        "                          (r\"\\bdidn't\\b\", \"did not\"),\n",
        "                          (r\"\\bhasn't\\b\", \"has not\"),\n",
        "                          (r\"\\bhaven't\\b\", \"have not\"),\n",
        "                          (r\"\\bhadn't\\b\", \"had not\"),\n",
        "                          (r\"\\bwon't\\b\", \"will not\"),\n",
        "                          (r\"\\bwouldn't\\b\", \"would not\"),\n",
        "                          (r\"\\bcan't\\b\", \"can not\"),\n",
        "                          (r\"\\bcannot\\b\", \"can not\"),\n",
        "                          (r\"\\bi'm\\b\", \"i am\"),\n",
        "                          (\"m\", \"am\"),\n",
        "                          (\"r\", \"are\"),\n",
        "                          (\"u\", \"you\"),\n",
        "                          (\"haha\", \"ha\"),\n",
        "                          (\"hahaha\", \"ha\"),\n",
        "                          (\"don't\", \"do not\"),\n",
        "                          (\"doesn't\", \"does not\"),\n",
        "                          (\"didn't\", \"did not\"),\n",
        "                          (\"hasn't\", \"has not\"),\n",
        "                          (\"haven't\", \"have not\"),\n",
        "                          (\"hadn't\", \"had not\"),\n",
        "                          (\"won't\", \"will not\"),\n",
        "                          (\"wouldn't\", \"would not\"),\n",
        "                          (\"can't\", \"can not\"),\n",
        "                          (\"cannot\", \"can not\"),\n",
        "                          (\"i'm\", \"i am\"),\n",
        "                          (\"m\", \"am\"),\n",
        "                          (\"i'll\" , \"i will\"),\n",
        "                          (\"its\" , \"it is\"),\n",
        "                          (\"it's\" , \"it is\"),\n",
        "                          (\"'s\" , \" is\"),\n",
        "                          (\"that's\" , \"that is\"),\n",
        "                          (\"weren't\" , \"were not\")\n",
        "    ]\n",
        "\n",
        "    # Clean the text, with the option to remove stop_words and to stem words.\n",
        "    text                = text.lower()\n",
        "\n",
        "    for t in NEG_CONTRACTIONS:\n",
        "            text        = re.sub(t[0], t[1], text)\n",
        "\n",
        "\n",
        "    for t in OTHER_CONTRACTIONS:\n",
        "            text        = re.sub(t[0], t[1], text)\n",
        "    for t in OTHER_RPS:\n",
        "            #print(t)\n",
        "            text.replace(t[0], t[1])\n",
        "            #text = re.sub(t[0], t[1], text)\n",
        "\n",
        "    # Clean the text\n",
        "    if with_punct_sent:\n",
        "      text              = re.sub(r\"[^A-Za-z0-9!.?]\", \" \", text)\n",
        "    else:\n",
        "      text              = re.sub(r\"[^A-Za-z0-9]\", \" \", text)\n",
        "\n",
        "    text                = re.sub(r\"what's\", \"\", text)\n",
        "    #text               = re.sub(r\"What's\", \"\", text)\n",
        "    text                = re.sub(r\"\\'s\", \" \", text)\n",
        "    text                = re.sub(r\"\\'ve\", \" have \", text)\n",
        "    text                = re.sub(r\"can\\'t\", \"cannot \", text)\n",
        "    text                = re.sub(r\"n\\'t\", \" not \", text)\n",
        "    text                = re.sub(r\"i\\'m\", \"i am\", text)\n",
        "    text                = re.sub(r\" m \", \" am \", text)\n",
        "    text                = re.sub(r\"\\'re\", \" are \", text)\n",
        "    text                = re.sub(r\"\\'d\", \" would \", text)\n",
        "    text                = re.sub(r\"\\'ll\", \" will \", text)\n",
        "    text                = re.sub('[0-9]+', '', text)\n",
        "\n",
        "\n",
        "\n",
        "    #if with_punct_sent==False:\n",
        "    #    pass\n",
        "        #text = ''.join([c for c in text if c not in '!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'])\n",
        "    #else:\n",
        "     #   text = ''.join([c for c in text if c not in '!.?'])\n",
        "\n",
        "    # Optionally, remove stop words\n",
        "    if remove_stop_words:\n",
        "        text           = text.split()\n",
        "        text           = [w for w in text if not w in stop_words]\n",
        "        text           = \" \".join(text)\n",
        "\n",
        "    # Optionally, shorten words to their stems\n",
        "    if stem_words:\n",
        "        text           = text.split()\n",
        "        stemmer        = SnowballStemmer('english')\n",
        "        stemmed_words  = [stemmer.stem(word) for word in text]\n",
        "        text           = \" \".join(stemmed_words)\n",
        "\n",
        "    ltr = text.split()\n",
        "    new_data = []\n",
        "    for i in ltr:\n",
        "        arr = str(i).split()\n",
        "        xx = \"\"\n",
        "        for j in arr:\n",
        "            j = str(j).lower()\n",
        "            if j[:4] == 'http' or j[:3] == 'www':\n",
        "                continue\n",
        "            xx += j + ' '\n",
        "        new_data.append(xx)\n",
        "    text = ''.join(new_data)\n",
        "    return(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKQkh-EkSZyn",
        "outputId": "7b2d63e6-6b53-4f22-ad0d-6446042be8d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing the data without any punctuation\n",
            "Train\n",
            "Test labelled\n",
            "Test unlabelled\n"
          ]
        }
      ],
      "source": [
        "print('Preparing the data without any punctuation')\n",
        "train_no_punct                            = train.copy()\n",
        "test_labelled_no_punct                    = test_labelled.copy()\n",
        "test_unlabelled_no_punct                  = test_unlabelled.copy()\n",
        "\n",
        "print('Train')\n",
        "train_no_punct.loc[:,'comment_text']            = train['comment_text'].apply(lambda x : text_to_wordlist(x, remove_stop_words=True, stem_words=False))\n",
        "print('Test labelled')\n",
        "test_labelled_no_punct.loc[:,'comment_text']    = test_labelled['comment_text'].apply(lambda x : text_to_wordlist(x, remove_stop_words=True, stem_words=False))\n",
        "print('Test unlabelled')\n",
        "test_unlabelled_no_punct.loc[:,'comment_text']  = test_unlabelled['comment_text'].apply(lambda x : text_to_wordlist(x, remove_stop_words=True, stem_words=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rikI5ZSYUltl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Create folder inside Colab\n",
        "data_path = '/content/drive/MyDrive/SJ-DECEN_code/data/'\n",
        "os.makedirs(data_path, exist_ok=True)\n",
        "\n",
        "# Save your files\n",
        "train_no_punct.to_csv(f'{data_path}/train_cleaned_no_punkt.csv', index=False)\n",
        "test_labelled_no_punct.drop('used_for_sc', axis=1).to_csv(f'{data_path}/test_labelled_cleaned_no_punkt.csv', index=False)\n",
        "test_unlabelled_no_punct.drop('used_for_sc', axis=1).to_csv(f'{data_path}/test_unlabelled_cleaned_no_punkt.csv', index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoqeUel9VWOx",
        "outputId": "882e033b-b654-41e0-ba22-0e96619a1858"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing the data with sentence punctuation\n",
            "Train\n",
            "Test labelled\n",
            "Test unlabelled\n"
          ]
        }
      ],
      "source": [
        "print('Preparing the data with sentence punctuation')\n",
        "train_sent_punct = train.copy()\n",
        "test_labelled_sent_punct = test_labelled.copy()\n",
        "test_unlabelled_sent_punct = test_unlabelled.copy()\n",
        "\n",
        "print('Train')\n",
        "train_sent_punct.loc[:,'comment_text']            = train['comment_text'].apply(lambda x : text_to_wordlist(x, remove_stop_words=True, stem_words=False, with_punct_sent=True))\n",
        "print('Test labelled')\n",
        "test_labelled_sent_punct.loc[:,'comment_text']    = test_labelled['comment_text'].apply(lambda x : text_to_wordlist(x, remove_stop_words=True, stem_words=False, with_punct_sent=True))\n",
        "print('Test unlabelled')\n",
        "test_unlabelled_sent_punct.loc[:,'comment_text']  = test_unlabelled['comment_text'].apply(lambda x : text_to_wordlist(x, remove_stop_words=True, stem_words=False, with_punct_sent=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bGjZHk_vV7p0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Create a folder in Colab's /SJ-DECEN_code directory\n",
        "data_path = '/content/drive/MyDrive/SJ-DECEN_code/data/'\n",
        "os.makedirs(data_path, exist_ok=True)\n",
        "\n",
        "# Save the files\n",
        "train_sent_punct.to_csv(f'{data_path}/train_cleaned_sent_punkt.csv', index=False)\n",
        "test_labelled_sent_punct.drop('used_for_sc', axis=1).to_csv(f'{data_path}/test_labelled_cleaned_sent_punkt.csv', index=False)\n",
        "test_unlabelled_sent_punct.drop('used_for_sc', axis=1).to_csv(f'{data_path}/test_unlabelled_cleaned_sent_punkt.csv', index=False)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}